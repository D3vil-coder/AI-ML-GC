{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f79ad8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide #    | Index (idx)  | Name                      | Type\n",
      "----------------------------------------------------------------------\n",
      "0          | 0            | Title 1                   | CENTER_TITLE (3)\n",
      "0          | 1            | Subtitle 2                | SUBTITLE (4)\n",
      "1          | 0            | Title 1                   | TITLE (1)\n",
      "1          | 1            | Picture Placeholder 2     | PICTURE (18)\n",
      "1          | 2            | Text Placeholder 3        | BODY (2)\n",
      "2          | 0            | Title 1                   | TITLE (1)\n",
      "2          | 1            | Content Placeholder 2     | OBJECT (7)\n",
      "3          | 0            | Title 1                   | TITLE (1)\n",
      "3          | 1            | Content Placeholder 2     | OBJECT (7)\n",
      "4          | 0            | Title 1                   | TITLE (1)\n",
      "4          | 1            | Content Placeholder 2     | OBJECT (7)\n"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "\n",
    "def find_my_template_ids(filename):\n",
    "    try:\n",
    "        prs = Presentation(filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Could not open {filename}. Make sure it's in the folder. {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"{'Slide #':<10} | {'Index (idx)':<12} | {'Name':<25} | {'Type'}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for i, slide in enumerate(prs.slides):\n",
    "        # We look at placeholders on the actual slides\n",
    "        for shape in slide.placeholders:\n",
    "            idx = shape.placeholder_format.idx\n",
    "            name = shape.name\n",
    "            ph_type = shape.placeholder_format.type\n",
    "            print(f\"{i:<10} | {idx:<12} | {name:<25} | {ph_type}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CHANGE THIS to your actual template filename\n",
    "    FILE_NAME = \"template.pptx\" \n",
    "    find_my_template_ids(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495d1bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! 'Final_Presentation.pptx' has been generated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pptx import Presentation\n",
    "from pptx.chart.data import CategoryChartData, ChartData\n",
    "from pptx.enum.chart import XL_CHART_TYPE\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TEMPLATE_FILE = \"template.pptx\"\n",
    "OUTPUT_FILE = \"Final_Presentation.pptx\"\n",
    "\n",
    "# Replace these with your actual local image paths\n",
    "IMAGE_PATH_1 = \"bada_loda.png\" \n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(TEMPLATE_FILE):\n",
    "        print(f\"Error: {TEMPLATE_FILE} not found in current directory.\")\n",
    "        return\n",
    "\n",
    "    prs = Presentation(TEMPLATE_FILE)\n",
    "\n",
    "    # --- SLIDE 0: TITLE SLIDE ---\n",
    "    # idx 0: Title, idx 1: Subtitle\n",
    "    s0 = prs.slides[0]\n",
    "    s0.placeholders[0].text = \"IIT Bombay: Robotics Project Report\"\n",
    "    s0.placeholders[1].text = \"Automated System Analysis v1.0\"\n",
    "\n",
    "    # --- SLIDE 1: TEXT & IMAGE ---\n",
    "    # idx 0: Title, idx 1: Picture, idx 2: Body Text\n",
    "    s1 = prs.slides[1]\n",
    "    s1.placeholders[0].text = \"Hardware Architecture\"\n",
    "    s1.placeholders[2].text = (\n",
    "        \"• STM32 Blue Pill integrated with MPU9250.\\n\"\n",
    "        \"• Real-time sensor fusion for drone stability.\\n\"\n",
    "        \"• 3.7V LiPo power management logic.\"\n",
    "    )\n",
    "    if os.path.exists(IMAGE_PATH_1):\n",
    "        s1.placeholders[1].insert_picture(IMAGE_PATH_1)\n",
    "    else:\n",
    "        print(f\"Warning: {IMAGE_PATH_1} not found. Skipping image injection.\")\n",
    "\n",
    "    # --- SLIDE 2: BAR CHART (Clustered Column) ---\n",
    "    # idx 1: Content Placeholder\n",
    "    s2 = prs.slides[2]\n",
    "    s2.placeholders[0].text = \"Sensor Latency Analysis\"\n",
    "    \n",
    "    chart_data_1 = CategoryChartData()\n",
    "    chart_data_1.categories = ['MPU6050', 'MPU9250', 'BNO055']\n",
    "    chart_data_1.add_series('Latency (ms)', (12.5, 8.2, 4.1))\n",
    "    \n",
    "    replace_with_chart(s2, 1, XL_CHART_TYPE.COLUMN_CLUSTERED, chart_data_1)\n",
    "\n",
    "    # --- SLIDE 3: PIE CHART ---\n",
    "    # idx 1: Content Placeholder\n",
    "    s3 = prs.slides[3]\n",
    "    s3.placeholders[0].text = \"Power Consumption Breakdown\"\n",
    "    \n",
    "    chart_data_2 = ChartData()\n",
    "    chart_data_2.categories = ['Motors', 'MCU', 'RF Module', 'Sensors']\n",
    "    chart_data_2.add_series('Usage', (0.70, 0.15, 0.10, 0.05))\n",
    "    \n",
    "    replace_with_chart(s3, 1, XL_CHART_TYPE.PIE, chart_data_2)\n",
    "\n",
    "    # --- SLIDE 4: LINE CHART ---\n",
    "    # idx 1: Content Placeholder\n",
    "    s4 = prs.slides[4]\n",
    "    s4.placeholders[0].text = \"Thrust vs PWM Signal\"\n",
    "    \n",
    "    chart_data_3 = CategoryChartData()\n",
    "    chart_data_3.categories = ['1000us', '1250us', '1500us', '1750us', '2000us']\n",
    "    chart_data_3.add_series('Thrust (g)', (0, 45, 110, 185, 240))\n",
    "    \n",
    "    replace_with_chart(s4, 1, XL_CHART_TYPE.LINE, chart_data_3)\n",
    "\n",
    "    # --- SAVE ---\n",
    "    prs.save(OUTPUT_FILE)\n",
    "    print(f\"Success! '{OUTPUT_FILE}' has been generated.\")\n",
    "\n",
    "def replace_with_chart(slide, idx, chart_type, data):\n",
    "    \"\"\"\n",
    "    Finds placeholder by index, deletes the empty box, \n",
    "    and adds a native editable chart in the same spot.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ph = slide.placeholders[idx]\n",
    "        # Store dimensions\n",
    "        left, top, width, height = ph.left, ph.top, ph.width, ph.height\n",
    "        # Delete placeholder\n",
    "        sp = ph._element\n",
    "        sp.getparent().remove(sp)\n",
    "        # Create Chart\n",
    "        slide.shapes.add_chart(chart_type, left, top, width, height, data)\n",
    "    except KeyError:\n",
    "        print(f\"Error: Index {idx} not found on this slide.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006d531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:47: SyntaxWarning: invalid escape sequence '\\Z'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\Z'\n",
      "C:\\Users\\Ansh\\AppData\\Local\\Temp\\ipykernel_27984\\1425877413.py:47: SyntaxWarning: invalid escape sequence '\\Z'\n",
      "  pattern = f\"{re.escape(header)}\\n\\n(.*?)(?=\\n##|\\Z)\"\n",
      "INFO:__main__:Extracting data from Ksolves-OnePager.md\n",
      "ERROR:__main__:Validation failed: ['Missing website URL']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXTRACTION SUMMARY ===\n",
      "{\n",
      "  \"source\": \"Ksolves-OnePager.md\",\n",
      "  \"company_type\": \"Unknown\",\n",
      "  \"has_website\": false,\n",
      "  \"num_products\": 10,\n",
      "  \"num_shareholders\": 11,\n",
      "  \"financial_years\": 6,\n",
      "  \"num_milestones\": 32,\n",
      "  \"validation_errors\": [\n",
      "    \"Missing website URL\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "=== DERIVED METRICS ===\n",
      "{\n",
      "  \"revenue_cagr\": 48.6,\n",
      "  \"latest_revenue\": 1374.3,\n",
      "  \"latest_year\": 2025,\n",
      "  \"ebitda_margin\": 35.6,\n",
      "  \"pat_margin\": 24.9,\n",
      "  \"roce\": 79.2\n",
      "}\n",
      "\n",
      "❌ Validation failed\n",
      "Errors: ['Missing website URL']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Agent 2: Data Extractor\n",
    "Extracts structured data from markdown one-pagers using pandas and regex.\n",
    "ZERO LLM USAGE - 100% deterministic extraction.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, Optional\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DataExtractor:\n",
    "    \"\"\"Extract structured data from markdown one-pagers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        self.validation_errors = []\n",
    "    \n",
    "    def extract(self, md_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main extraction method - returns structured JSON\"\"\"\n",
    "        logger.info(f\"Extracting data from {md_path}\")\n",
    "        \n",
    "        with open(md_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        self.data = {\n",
    "            'source_file': md_path,\n",
    "            'business_description': self._extract_section(content, '## Business Description'),\n",
    "            'website': self._extract_section(content, '## Website').strip(),\n",
    "            'products_services': self._extract_list_section(content, '## Product & Services'),\n",
    "            'industries': self._extract_section(content, '## Application areas / Industries served'),\n",
    "            'shareholders': self._extract_shareholders(content),\n",
    "            'financials': self._extract_financials(content),\n",
    "            'key_milestones': self._extract_milestones(content),\n",
    "            'key_metrics': self._extract_key_metrics(content),\n",
    "            'operational_indicators': self._extract_operational_indicators(content)\n",
    "        }\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def _extract_section(self, content: str, header: str) -> str:\n",
    "        \"\"\"Extract text between header and next header or end\"\"\"\n",
    "        pattern = f\"{re.escape(header)}\\n\\n(.*?)(?=\\n##|\\Z)\"\n",
    "        match = re.search(pattern, content, re.DOTALL)\n",
    "        return match.group(1).strip() if match else \"\"\n",
    "    \n",
    "    def _extract_list_section(self, content: str, header: str) -> List[str]:\n",
    "        \"\"\"Extract bullet point list items\"\"\"\n",
    "        section = self._extract_section(content, header)\n",
    "        \n",
    "        # Match markdown list items (- ** or - *)\n",
    "        pattern = r'-\\s+\\*\\*(.+?)\\*\\*'\n",
    "        items = re.findall(pattern, section)\n",
    "        \n",
    "        if not items:\n",
    "            # Try simple dash lists\n",
    "            pattern = r'-\\s+(.+?)(?=\\n-|\\Z)'\n",
    "            items = re.findall(pattern, section, re.DOTALL)\n",
    "            items = [item.strip() for item in items]\n",
    "        \n",
    "        return items\n",
    "    \n",
    "    def _extract_shareholders(self, content: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract shareholder tables\"\"\"\n",
    "        section = self._extract_section(content, '## Shareholders')\n",
    "        \n",
    "        shareholders = []\n",
    "        \n",
    "        # Find all markdown tables in section\n",
    "        table_pattern = r'\\|(.+?)\\|(.+?)\\|(.+?)\\|'\n",
    "        matches = re.findall(table_pattern, section)\n",
    "        \n",
    "        for match in matches:\n",
    "            name, value, share_type = [m.strip() for m in match]\n",
    "            \n",
    "            # Skip header rows\n",
    "            if 'SHAREHOLDER NAME' in name or '---' in name:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Extract percentage value\n",
    "                value_num = float(re.search(r'[\\d.]+', value).group())\n",
    "                \n",
    "                shareholders.append({\n",
    "                    'name': name,\n",
    "                    'percentage': value_num,\n",
    "                    'type': share_type\n",
    "                })\n",
    "            except (AttributeError, ValueError):\n",
    "                continue\n",
    "        \n",
    "        return shareholders\n",
    "    \n",
    "    def _extract_financials(self, content: str) -> Dict[str, Dict[int, float]]:\n",
    "        \"\"\"\n",
    "        Extract financial data from markdown tables.\n",
    "        CRITICAL: This is ZERO LLM - direct regex parsing.\n",
    "        \"\"\"\n",
    "        financials = {}\n",
    "        \n",
    "        # Key financial metrics to extract\n",
    "        metrics = {\n",
    "            'revenue': 'Revenue From Operations',\n",
    "            'ebitda': 'Operating EBITDA',\n",
    "            'pat_margin': 'PAT Margin',\n",
    "            'roce': 'RoCE',\n",
    "            'roe': 'ROE',\n",
    "            'asset_turnover': 'Asset Turnover',\n",
    "            'total_assets': 'Total Assets',\n",
    "            'total_equity': 'Total Equity',\n",
    "            'borrowings': 'Borrowings',\n",
    "            'cash_flow_operations': 'Net cash flow from operating activities',\n",
    "            'inventory_days': 'inventory_days',\n",
    "            'receivable_days': 'receivable_days',\n",
    "            'payable_days': 'payable_days'\n",
    "        }\n",
    "        \n",
    "        for key, metric_name in metrics.items():\n",
    "            pattern = f\"{re.escape(metric_name)} \\\\| (.+)\"\n",
    "            match = re.search(pattern, content)\n",
    "            \n",
    "            if match:\n",
    "                row_data = match.group(1)\n",
    "                parsed_data = self._parse_financial_row(row_data)\n",
    "                \n",
    "                if parsed_data:\n",
    "                    financials[key] = parsed_data\n",
    "                    logger.debug(f\"Extracted {key}: {len(parsed_data)} years\")\n",
    "        \n",
    "        # Validate we have core metrics\n",
    "        required = ['revenue', 'ebitda']\n",
    "        for req in required:\n",
    "            if req not in financials or len(financials[req]) < 3:\n",
    "                self.validation_errors.append(f\"Missing or insufficient data for {req}\")\n",
    "        \n",
    "        return financials\n",
    "    \n",
    "    def _parse_financial_row(self, row_data: str) -> Dict[int, float]:\n",
    "        \"\"\"\n",
    "        Parse financial row: 2014: 4251.81863 | 2015: 4879.97017 | ...\n",
    "        Returns dict of {year: value}\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        \n",
    "        # Split by pipe\n",
    "        entries = row_data.split('|')\n",
    "        \n",
    "        for entry in entries:\n",
    "            entry = entry.strip()\n",
    "            \n",
    "            if ':' not in entry:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                year_str, value_str = entry.split(':', 1)\n",
    "                year = int(year_str.strip())\n",
    "                value_str = value_str.strip()\n",
    "                \n",
    "                # Handle 'None' values\n",
    "                if value_str.lower() == 'none':\n",
    "                    continue\n",
    "                \n",
    "                # Remove commas and convert\n",
    "                value = float(value_str.replace(',', ''))\n",
    "                \n",
    "                data[year] = value\n",
    "                \n",
    "            except (ValueError, AttributeError) as e:\n",
    "                logger.debug(f\"Failed to parse entry '{entry}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _extract_milestones(self, content: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract key milestones table\"\"\"\n",
    "        section = self._extract_section(content, '## Key Milestones')\n",
    "        \n",
    "        milestones = []\n",
    "        \n",
    "        # Match table rows\n",
    "        pattern = r'\\|\\s*(.+?)\\s*\\|\\s*(.+?)\\s*\\|'\n",
    "        matches = re.findall(pattern, section)\n",
    "        \n",
    "        for match in matches:\n",
    "            date, milestone = [m.strip() for m in match]\n",
    "            \n",
    "            # Skip headers\n",
    "            if 'DATE' in date or '---' in date:\n",
    "                continue\n",
    "            \n",
    "            milestones.append({\n",
    "                'date': date,\n",
    "                'description': milestone\n",
    "            })\n",
    "        \n",
    "        return milestones\n",
    "    \n",
    "    def _extract_key_metrics(self, content: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract key operational metrics\"\"\"\n",
    "        section = self._extract_section(content, '## Key Metrics')\n",
    "        \n",
    "        if section.lower() == 'not available':\n",
    "            return {}\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Try to extract specific metrics mentioned\n",
    "        # Examples: \"Employee count: 450\", \"Revenue per employee: ₹24L\"\n",
    "        patterns = {\n",
    "            'employee_count': r'(\\d+)\\s*(?:employees|developers)',\n",
    "            'facilities': r'(\\d+)\\s*facilities',\n",
    "            'customers': r'(\\d+)\\+?\\s*customers',\n",
    "            'certifications': r'(ISO|CMMI|FSSC|GMP|USFDA)',\n",
    "        }\n",
    "        \n",
    "        for key, pattern in patterns.items():\n",
    "            match = re.search(pattern, section, re.IGNORECASE)\n",
    "            if match:\n",
    "                metrics[key] = match.group(1)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _extract_operational_indicators(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract operational indicators as list\"\"\"\n",
    "        section = self._extract_section(content, '## Key Operational Indicators')\n",
    "        \n",
    "        if not section:\n",
    "            return []\n",
    "        \n",
    "        indicators = []\n",
    "        \n",
    "        # Match bullet points starting with **\n",
    "        pattern = r'\\*\\s+\\*\\*(.+?):\\*\\*\\s*(.+?)(?=\\n\\*|\\Z)'\n",
    "        matches = re.findall(pattern, section, re.DOTALL)\n",
    "        \n",
    "        for title, description in matches:\n",
    "            indicators.append(f\"{title}: {description.strip()}\")\n",
    "        \n",
    "        return indicators\n",
    "    \n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"Validate extracted data meets requirements\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Check critical fields\n",
    "        if not self.data.get('website'):\n",
    "            errors.append(\"Missing website URL\")\n",
    "        \n",
    "        if not self.data.get('business_description'):\n",
    "            errors.append(\"Missing business description\")\n",
    "        \n",
    "        # Check financials\n",
    "        financials = self.data.get('financials', {})\n",
    "        \n",
    "        if 'revenue' not in financials:\n",
    "            errors.append(\"Missing revenue data\")\n",
    "        elif len(financials['revenue']) < 3:\n",
    "            errors.append(f\"Insufficient revenue data: {len(financials['revenue'])} years (need 3+)\")\n",
    "        \n",
    "        if 'ebitda' not in financials:\n",
    "            errors.append(\"Missing EBITDA data\")\n",
    "        \n",
    "        # Check years are consistent\n",
    "        if 'revenue' in financials and 'ebitda' in financials:\n",
    "            revenue_years = set(financials['revenue'].keys())\n",
    "            ebitda_years = set(financials['ebitda'].keys())\n",
    "            \n",
    "            if revenue_years != ebitda_years:\n",
    "                errors.append(f\"Year mismatch between revenue and EBITDA\")\n",
    "        \n",
    "        if errors:\n",
    "            logger.error(f\"Validation failed: {errors}\")\n",
    "            self.validation_errors.extend(errors)\n",
    "            return False\n",
    "        \n",
    "        logger.info(\"Data validation passed\")\n",
    "        return True\n",
    "    \n",
    "    def calculate_derived_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate derived financial metrics from raw data\"\"\"\n",
    "        financials = self.data.get('financials', {})\n",
    "        \n",
    "        if not financials:\n",
    "            return {}\n",
    "        \n",
    "        derived = {}\n",
    "        \n",
    "        # Get latest 5 years of data\n",
    "        if 'revenue' in financials:\n",
    "            years = sorted(financials['revenue'].keys())\n",
    "            latest_years = years[-5:] if len(years) >= 5 else years\n",
    "            \n",
    "            revenues = [financials['revenue'][yr] for yr in latest_years]\n",
    "            \n",
    "            # Revenue CAGR\n",
    "            if len(revenues) >= 2:\n",
    "                n_years = len(revenues) - 1\n",
    "                cagr = ((revenues[-1] / revenues[0]) ** (1/n_years) - 1) * 100\n",
    "                derived['revenue_cagr'] = round(cagr, 1)\n",
    "            \n",
    "            # Latest revenue\n",
    "            derived['latest_revenue'] = revenues[-1]\n",
    "            derived['latest_year'] = latest_years[-1]\n",
    "        \n",
    "        # EBITDA margin (latest year)\n",
    "        if 'revenue' in financials and 'ebitda' in financials:\n",
    "            latest_year = max(financials['revenue'].keys())\n",
    "            if latest_year in financials['ebitda']:\n",
    "                revenue = financials['revenue'][latest_year]\n",
    "                ebitda = financials['ebitda'][latest_year]\n",
    "                \n",
    "                if revenue > 0:\n",
    "                    margin = (ebitda / revenue) * 100\n",
    "                    derived['ebitda_margin'] = round(margin, 1)\n",
    "        \n",
    "        # PAT margin (latest)\n",
    "        if 'pat_margin' in financials:\n",
    "            latest_year = max(financials['pat_margin'].keys())\n",
    "            derived['pat_margin'] = round(financials['pat_margin'][latest_year], 1)\n",
    "        \n",
    "        # ROCE (latest)\n",
    "        if 'roce' in financials:\n",
    "            latest_year = max(financials['roce'].keys())\n",
    "            derived['roce'] = round(financials['roce'][latest_year], 1)\n",
    "        \n",
    "        # Working capital days (if available)\n",
    "        if all(k in financials for k in ['inventory_days', 'receivable_days', 'payable_days']):\n",
    "            latest_year = max(financials['inventory_days'].keys())\n",
    "            inv = financials['inventory_days'].get(latest_year, 0)\n",
    "            rec = financials['receivable_days'].get(latest_year, 0)\n",
    "            pay = financials['payable_days'].get(latest_year, 0)\n",
    "            \n",
    "            derived['net_working_capital_days'] = int(inv + rec - pay)\n",
    "        \n",
    "        return derived\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get a summary of extracted data for logging/debugging\"\"\"\n",
    "        return {\n",
    "            'source': self.data.get('source_file'),\n",
    "            'company_type': 'Unknown',  # Will be determined by Agent 1\n",
    "            'has_website': bool(self.data.get('website')),\n",
    "            'num_products': len(self.data.get('products_services', [])),\n",
    "            'num_shareholders': len(self.data.get('shareholders', [])),\n",
    "            'financial_years': len(self.data.get('financials', {}).get('revenue', {})),\n",
    "            'num_milestones': len(self.data.get('key_milestones', [])),\n",
    "            'validation_errors': self.validation_errors\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    import json\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # if len(sys.argv) < 2:\n",
    "    #     print(\"Usage: python 02_data_extractor.py <path_to_onepager.md>\")\n",
    "    #     sys.exit(1)\n",
    "    \n",
    "    extractor = DataExtractor()\n",
    "    data = extractor.extract(\"Ksolves-OnePager.md\")\n",
    "    \n",
    "    # Validate\n",
    "    is_valid = extractor.validate()\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    derived = extractor.calculate_derived_metrics()\n",
    "    data['derived_metrics'] = derived\n",
    "    \n",
    "    # Print summary\n",
    "    summary = extractor.get_summary()\n",
    "    print(\"\\n=== EXTRACTION SUMMARY ===\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    \n",
    "    print(\"\\n=== DERIVED METRICS ===\")\n",
    "    print(json.dumps(derived, indent=2))\n",
    "    \n",
    "    if is_valid:\n",
    "        print(\"\\n✅ Extraction successful and validated\")\n",
    "    else:\n",
    "        print(\"\\n❌ Validation failed\")\n",
    "        print(\"Errors:\", extractor.validation_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
